{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example MAB from chatGPT for reference on working MAB algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMultiArmedBandit\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, k, true_means):\n",
    "        # k: number of arms\n",
    "        # true_means: expected reward of each arm\n",
    "        self.k = k\n",
    "        self.true_means = true_means\n",
    "        self.estimates = np.zeros(k)  # Estimated values of each arm\n",
    "        self.arm_counts = np.zeros(k)  # Counts of how many times each arm has been pulled\n",
    "\n",
    "    def get_reward(self, arm):\n",
    "        # Simulate reward by sampling from a normal distribution around the true mean of the arm\n",
    "        return np.random.normal(self.true_means[arm], 1)\n",
    "\n",
    "    def select_arm(self, t):\n",
    "        # UCB action selection\n",
    "        ucb_values = self.estimates + np.sqrt(2 * np.log(t + 1) / (self.arm_counts + 1e-5))\n",
    "        return np.argmax(ucb_values)  # Select the arm with the highest UCB value\n",
    "\n",
    "    def update_estimates(self, arm, reward):\n",
    "        # Update the estimated value of the arm based on new reward\n",
    "        self.arm_counts[arm] += 1\n",
    "        n = self.arm_counts[arm]\n",
    "        # Incremental formula for mean\n",
    "        self.estimates[arm] += (reward - self.estimates[arm]) / n\n",
    "\n",
    "def simulate_bandit(k, true_means, iterations):\n",
    "    bandit = MultiArmedBandit(k, true_means)\n",
    "    rewards = np.zeros(iterations)\n",
    "    for t in range(1, iterations + 1):\n",
    "        arm = bandit.select_arm(t)\n",
    "        reward = bandit.get_reward(arm)\n",
    "        bandit.update_estimates(arm, reward)\n",
    "        rewards[t - 1] = reward\n",
    "    return rewards, bandit.estimates\n",
    "\n",
    "def main():\n",
    "    k = 10  # Number of arms\n",
    "    true_means = np.random.normal(0, 1, k)  # True mean reward for each arm\n",
    "    iterations = 1000  # Number of rounds to simulate\n",
    "\n",
    "    rewards, estimated_means = simulate_bandit(k, true_means, iterations)\n",
    "\n",
    "    cumulative_rewards = np.cumsum(rewards)\n",
    "    average_rewards = cumulative_rewards / (np.arange(iterations) + 1)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(average_rewards, label='Average Reward')\n",
    "    plt.title('Multi-Armed Bandit (UCB)')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"True Means: \", true_means)\n",
    "    print(\"Estimated Means after simulation: \", estimated_means)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCB Algorithm: The core of the Upper Confidence Bound (UCB) is that for each arm, we compute a confidence bound on its mean based on how many times it has been selected. The algorithm then selects the arm with the highest upper bound, balancing the exploration of arms with fewer pulls (higher uncertainty) and exploiting arms that are known to give high rewards.\n",
    "The formula used for UCB is:\n",
    "\n",
    "UCB(subt)(a) = avgu(suba) + sqrt(2logt/N(suba)(t))\n",
    "\n",
    "where:\n",
    "avgu(suba) is the estimated mean of arm \n",
    "â€‹\t\n",
    " N(suba)(t) is the number of times arm a has been pulled,\n",
    "\n",
    "t is the current time step (iteration).\n",
    "\n",
    "This formula ensures that the arms with higher uncertainty (less pulls) are given a chance to be explored, while arms that have been pulled many times are exploited more often.\n",
    "Simulation: The simulation runs for iterations number of steps, and in each step, the algorithm selects the arm using the UCB strategy, receives a reward, and updates the estimated mean for that arm.\n",
    "Plot: After the simulation, we plot the average reward over time to observe how the agent's performance improves as it explores and exploits the arms.\n",
    "Expected Output:\n",
    "True Means: The true average reward for each arm (not known to the algorithm during the run).\n",
    "Estimated Means: The final estimates of the means after the simulation.\n",
    "Plot: A graph showing how the average reward evolves over time, with the goal of the agent getting closer to the optimal solution (maximizing the reward).\n",
    "This UCB approach tends to perform well in scenarios where the rewards of the arms have different levels of uncertainty, and it efficiently balances exploration and exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: -365.0195794476609\n"
     ]
    }
   ],
   "source": [
    "class UCB:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.total_counts = 0\n",
    "\n",
    "    def select_arm(self):\n",
    "        ucb_values = self.values + np.sqrt(2 * np.log(self.total_counts + 1) / (self.counts + 1e-5))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        self.total_counts += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "# Example usage\n",
    "\n",
    "n_arms = 19\n",
    "n_trials = 1000\n",
    "rewards = np.random.randn(n_arms, n_trials)\n",
    "\n",
    "agent = UCB(n_arms)\n",
    "\n",
    "\n",
    "for t in range(n_trials):\n",
    "    arm = agent.select_arm()\n",
    "    reward = rewards[arm, t]\n",
    "    agent.update(arm, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

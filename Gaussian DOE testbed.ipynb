{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import qmc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, Kernel, Hyperparameter\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate optimized LHS\n",
    "lhs = qmc.LatinHypercube(d=3)\n",
    "samples = lhs.random(n=24)\n",
    "\n",
    "#plt.scatter(samples[:, 0], samples[:, 1]) # for 2d \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2])\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "plt.title('lhs test')\n",
    "plt.show()\n",
    "\n",
    "# Print x, y, z values in a readable format\n",
    "print(\"X, Y, Z values:\")\n",
    "for i, (x, y, z) in enumerate(samples):\n",
    "    print(f\"Point {i+1}: x={x:.4f}, y={y:.4f}, z={z:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKernel(Kernel):\n",
    "    def __init__(self, variance = 1.0):\n",
    "        self.variance = variance # kernel variance\n",
    "        #define variance as a hyperparameter (log-scaled for positive values)\n",
    "        self.hyperparameter_variance = Hyperparameter(\"variance\", \"log\", 1e-2, 10.0)\n",
    "    def __call__(self, X, Y = None, eval_gradient = False):\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        \n",
    "        # Compute categorical overlap (1 if same, 0 if different)\n",
    "        diff = X[:, None] != Y[None, :] # Boolean matrix (True if different)\n",
    "        k_cat = self.variance * (1-np.mean(diff, axis=-1)) # normalizes overlap\n",
    "\n",
    "        if eval_gradient:\n",
    "            # Gradient w.r.t. variance\n",
    "            grad = np.ones_like(k_cat)[..., np.newaxis] # Shape: (n_samples, n_samples, 1)\n",
    "            return k_cat, grad\n",
    "        return k_cat\n",
    "    \n",
    "    def diag(self, X):\n",
    "        return np.full(X.shape[0], self.variance) # self-similarity\n",
    "    \n",
    "    def is_stationary(self):\n",
    "        return False\n",
    "    \n",
    "class CategoryOverlapKernelGPy(GPy.kern.Kern):\n",
    "    \"\"\"GPy implementation of a categorical overlap kernel.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, variance=1.0, active_dims=None, name='catoverlap'):\n",
    "        super().__init__(input_dim, active_dims=active_dims, name=name)\n",
    "        self.variance = GPy.core.parameterization.Param('variance', variance)\n",
    "        self.link_parameter(self.variance)\n",
    "\n",
    "    def K(self, X, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "\n",
    "        diff = X[:, None] - X2[None, :]\n",
    "        diff[np.where(np.abs(diff))] = 1  # Mark different categories\n",
    "        k_cat = self.variance * (1 - np.mean(diff, axis=-1))  # Normalize overlap\n",
    "        return k_cat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPy Kernel Matrix:\n",
      " [[1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]]\n",
      "SKL Kernel Matrix:\n",
      " [[1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([[0], [1], [2], [1], [0]]) #each row is a category\n",
    "\n",
    "# Create kernel and compute the kernel matrix\n",
    "gpy_kernel = CategoryOverlapKernelGPy(input_dim=1, variance=1.0)\n",
    "K_gpy = gpy_kernel.K(X_test)\n",
    "print(\"GPy Kernel Matrix:\\n\", K_gpy)\n",
    "\n",
    "#create kernel and compute the kernel matrix\n",
    "skl_kernel = CustomKernel(variance=1.0)\n",
    "K_skl = skl_kernel(X_test)\n",
    "print(\"SKL Kernel Matrix:\\n\", K_skl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKernel(Kernel):\n",
    "    \"\"\"Custom Kernel that adds a Matern Kernel with an extra exponential term.\"\"\"\n",
    "    \n",
    "    def __init__(self, length_scale=1.0, custom_param=1.0):\n",
    "        self.length_scale = length_scale\n",
    "        self.custom_param = custom_param\n",
    "    \n",
    "    def __call__(self, X, Y=None, eval_gradient=False):\n",
    "        \"\"\"Compute the kernel matrix.\"\"\"\n",
    "        dists = np.linalg.norm(X[:, np.newaxis] - Y[np.newaxis, :], axis=2)  # Euclidean distance\n",
    "        matern_kernel = Matern(length_scale=self.length_scale, nu=1.5)  # Matern Kernel (nu=1.5)\n",
    "        \n",
    "        # Custom Exponential Decay Function\n",
    "        custom_term = np.exp(-self.custom_param * dists)\n",
    "        \n",
    "        # Combine Matern with Custom Kernel\n",
    "        K = matern_kernel(X, Y) * custom_term  # Element-wise multiplication\n",
    "\n",
    "        if eval_gradient:\n",
    "            # Compute gradient w.r.t. hyperparameters\n",
    "            matern_grad = matern_kernel(X, Y, eval_gradient=True)[1]  # Get gradient from Matern\n",
    "            custom_grad = -dists * custom_term[:, :, np.newaxis]  # Derivative w.r.t custom_param\n",
    "            \n",
    "            # Stack gradients together\n",
    "            return K, np.dstack((matern_grad, custom_grad))\n",
    "        return K\n",
    "\n",
    "    def diag(self, X):\n",
    "        \"\"\"Diagonal elements (self-similarity should be 1).\"\"\"\n",
    "        return np.ones(X.shape[0])\n",
    "\n",
    "    def is_stationary(self):\n",
    "        \"\"\"Returns whether the kernel is stationary.\"\"\"\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = 1.0\n",
    "nu = 2.5 # i think default is 1.5, but we follow Narayana here who uses 2.5\n",
    "custom_param = 1.0\n",
    "\n",
    "matern_kernel = Matern(length_scale= length_scale, nu= nu)\n",
    "custom_kernel = CustomKernel(length_scale= length_scale, custom_param= custom_param)\n",
    "\n",
    "combined_kernel = # alpha * (matern_kernel*custom_kernel) + (1-alpha) * matern_kernel + custom_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = combined_kernel #(length_scale=1.0, custom_param=1.0)  # Initialize Custom Kernel\n",
    "alpha = 0.1 # noise level, default 1e-10 which assumes y value has no noise\n",
    "# I've set this to 0.1 but it can go higher for noisier data, also if noise variance is 0.1, try alpha =0.01\n",
    "optmizer = 'fmin_l_bfgs_b' # default optimizer, can also use 'fmin_l_bfgs_b' or 'fmin_cobyla'\n",
    "\n",
    "X = samples\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel = kernel, alpha = alpha, optimizer = optmizer, random_state = 42)\n",
    "gpr.fit(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom GPR with Matern kernel\n",
    "custom_gpr = GaussianProcessRegressor(kernel=Matern(length_scale=1.0, nu=1.5))\n",
    "\n",
    "# Run Bayesian Optimization with UCB using custom GPR\n",
    "res = gp_minimize(objective_function, space, n_calls=20, base_estimator=custom_gpr, \n",
    "                  acq_func=\"LCB\", acq_func_kwargs={\"kappa\": 2.0}, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnzymeKineticGPR-62lwjgN4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
